{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "\n",
    "pref_code = [46] #都道府県コード\n",
    "block_code = [1008] #地域コード\n",
    "place_name = [\"小田原\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBファイルを保存するためのファイルパス\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Github\n",
    "path = '/content/'\n",
    "\n",
    "# DBファイル名\n",
    "db_name = 'github.sqlite'\n",
    "\n",
    "# DBに接続する（指定したDBファイル存在しない場合は，新規に作成される）\n",
    "con = sqlite3.connect(path + db_name)\n",
    "cur = con.cursor()\n",
    "\n",
    "# テーブルが存在しない場合は作成\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS weather\n",
    "                ( TEXT, language TEXT, star INTEGER)''')\n",
    "\n",
    "# DBへの接続を閉じる\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.data.jma.go.jp/obd/stats/etrn/view/daily_a1.php?prec_no=%s&block_no%s&year=%s&month=%s&day=17&view=p1\"\n",
    "\n",
    "# 取ったデータをfloat型に変える(データが取れなかった場合、気象庁は\"/\"を埋め込んでいるため0に変える)\n",
    "def str2float(str):\n",
    "  try:\n",
    "    return float(str)\n",
    "  except:\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for place in place_name:\n",
    "    # 最終的にデータを集めるリスト\n",
    "        list = [['年月日', '降水量', '気温_平均', '気温_最高', '気温_最低', '湿度_平均', '湿度_最小', '日照時間']]\n",
    "        print(place)\n",
    "        index = place_name.index(place)\n",
    "\n",
    "    # 対象期間の年月を指定\n",
    "    year = 2023\n",
    "    print(year)\n",
    "    month = 12\n",
    "    #2つの都市コードと年と月を当てはめる。\n",
    "    r = requests.get(url%(pref_code[index], block_code[index], year, month))\n",
    "    r.encoding = r.apparent_encoding\n",
    "\n",
    "    # サイトごとスクレイピング\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    # findAllで条件に一致するものをすべて抜き出す。\n",
    "    # 今回の条件はtrタグでclassがmtxになっているもの。\n",
    "    rows = soup.findAll('tr',class_='mtx')\n",
    "\n",
    "    # 表の最初の1~4行目はカラム情報なのでスライスする。\n",
    "    rows = rows[4:]\n",
    "\n",
    "    # 対象期間の1行を取得\n",
    "    for row in rows:\n",
    "        # trのなかのtdをすべて抜き出す\n",
    "        data = row.findAll('td')\n",
    "\n",
    "        #1行の中のデータを全部取り出す。\n",
    "        rowData = [] #初期化\n",
    "        rowData.append(str(year) + \"/\" + str(month) + \"/\" + str(data[0].string))\n",
    "        rowData.append(str2float(data[3].string))\n",
    "        rowData.append(str2float(data[6].string))\n",
    "        rowData.append(str2float(data[7].string))\n",
    "        rowData.append(str2float(data[8].string))\n",
    "        rowData.append(str2float(data[9].string))\n",
    "        rowData.append(str2float(data[10].string))\n",
    "        rowData.append(str2float(data[16].string)) \n",
    "\n",
    "        #次の行にデータを追加\n",
    "        list.append(rowData)\n",
    "\n",
    "    #ファイルを生成(csvファイル形式。名前は都市名)\n",
    "    with open(place + '.csv', 'w',encoding=\"utf_8_sig\") as file: #文字化け防止\n",
    "      writer = csv.writer(file, lineterminator='\\n')\n",
    "      writer.writerows(list)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
